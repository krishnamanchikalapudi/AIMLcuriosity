{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLAMA2-7b-Chat\n\n- https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/\n- https://huggingface.co/meta-llama/Llama-2-7b-chat-hf","metadata":{}},{"cell_type":"code","source":"!pip install -q ipython-autotime torch transformers accelerate protobuf torch huggingface_hub[cli]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autotime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Login to HuggingFace to download the model\n- https://huggingface.co/meta-llama/Llama-2-7b-chat-hf","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the name of the model we'll use for the rest of the notebook\nmodel_name = \"meta-llama/Llama-2-7b-chat-hf\"\n\nimport transformers as transf\n\n# Load the base model\nmodel = transf.LlamaForCausalLM.from_pretrained(model_name)\n# model = transf.AutoModelForCausalLM.from_pretrained(model_name)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers as transf\n# Load the model tokenizer\ntokenizer = transf.LlamaTokenizer.from_pretrained(model_name)\ntokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a text generation pipeline which use the model and the tokenizer loaded\ngenerator = transf.pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\ngenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# default prompt\nprompt = \"\"\"<s>[INST] Hi, Are you there? How are you?  [/INST] \"\"\"\nresponse = generator(prompt, max_new_tokens=100)\nresponse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model folder information","metadata":{}},{"cell_type":"code","source":"import glob\nfor file in glob.iglob('/kaggle/working', recursive=True):\n    print(file)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(os.path.join(dirname))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Formated response","metadata":{}},{"cell_type":"code","source":"import textwrap\n\ndef display_response(prompt, generated_response, max_width=120):\n    # Function to print a bordered text box\n    def print_boxed(text):\n        lines = textwrap.wrap(text, max_width)  # Wrap text to desired width\n        border = '+' + '-' * (max_width + 2) + '+'\n        print(border)\n        for line in lines:\n            print('| ' + line.ljust(max_width) + ' |')\n        print(border)\n\n    # Extract the instruction and the patient's query from the prompt\n    instruction_start = prompt.find(\"[INST]\") + len(\"[INST]\")\n    instruction_end = prompt.find(\"[/INST]\")\n    instruction = prompt[instruction_start:instruction_end].strip()\n\n    prefix = \"As a medical doctor, respond to this patient query: Patient: \"\n    if instruction.startswith(prefix):\n        instruction = instruction[len(prefix):].strip()\n\n    # Extract the generated text from the response dictionary\n    response_text = generated_response[0]['generated_text']\n\n    # Extract the medical doctor's response from the generated text\n    doctor_response_start = response_text.find(\"[/INST]\") + len(\"[/INST]\")\n    doctor_response = response_text[doctor_response_start:].strip()\n\n    # Display the information with a wrapper\n    print(\"Human:\")\n    print_boxed(instruction)\n    print(\"\\nAssistance:\")\n    print_boxed(doctor_response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<s>[INST] Hi, Are you there? How are you?  [/INST] \"\"\"\nresponse = generator(prompt, max_new_tokens=100)\n\ndisplay_response(prompt, response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<s>[INST] I am quant trader. i would like take your help. how would you able to help me?  [/INST] \"\"\"\n\nresponse = generator(prompt, max_new_tokens=100)\ndisplay_response(prompt, response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<s>[INST] Ok. I am trying to develop a Random Forest Model to forecast next 5 day stock price returns based on past ohlc data & technical indicators. what all leading technical indicators i should use? [/INST] \"\"\"\n\n\nresponse = generator(prompt, max_new_tokens=100)\ndisplay_response(prompt, response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<s>[INST] Ok. I am trying to develop a Random Forest Model to forecast next 5 day stock price returns based on past ohlc data & technical indicators. what all technical indicators like CMF, RSI, OBV, stochostic, etc.. that leads the price action or returns? [/INST] \"\"\"\n\nresponse = generator(prompt, max_new_tokens=1000)\ndisplay_response(prompt, response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<s>[INST] Could you help me implement your above suggestion in python using yfinance as source for data? [/INST] \"\"\"\n\nresponse = generator(prompt, max_new_tokens=1000)\ndisplay_response(prompt, response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"<s>[INST] I am travelling to Las Vegas. could you help me with best shows in Vegas [/INST] \"\"\"\n\nresponse = generator(prompt, max_new_tokens=1000)\ndisplay_response(prompt, response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}